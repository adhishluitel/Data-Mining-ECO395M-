library(ggplot2)
library(foreach)
library(tidyverse)
library(LICORS)
library(cluster)
library(corrplot)
library(GGally)
csocial_marketing <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv", row.names = 1)
soma<- social_marketing
social_marketing <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv", row.names = 1)
rm(csocial_marketing)
soma<- social_marketing
#Cleaning the data
soma<-soma[(soma$spam==0),]
soma <- soma[,-35]
soma<- soma[,-c(1,5)]
soma <- cbind(tweet_sum = rowSums(soma), soma)
soma <- cbind(adult_ratio = 1, soma)
soma$adult_ratio <- soma$adult/soma$tweet_sum
soma<-soma[(soma$adult_ratio<0.2),]
soma <- soma[,-c(1,2)]
# Get rid of spam users - Nobody tweets spam messages unless (s)he's a bot
soma<-soma[(soma$spam==0),] # delete observations related to spam
soma <- soma[,-35] # get rid of spam variable
# we also delete variables "chatter" and "uncategorized"
# because the company cannot get any information from these categories
soma<- soma[,-c(1,5)]
# But we cannot delete all the users in adult category because it can be their hobby
# Then how can we distinguish bots from users?
# We meausure adult ratio
soma <- cbind(tweet_sum = rowSums(soma), soma)
soma <- cbind(adult_ratio = 1, soma)
soma$adult_ratio <- soma$adult/soma$tweet_sum
summary(soma$adult_ratio)
hist(soma$adult_ratio)
# Though there's no absolute criterion, We'll set 20% as our criterion
# Normal people don't tweet porn in public
# And their other non-adult tweets might lead other users into porn
length(which(soma$adult_ratio>0.2))
length(which(soma$adult_ratio>0.2))/(length(which(soma$adult_ratio>0)))
soma<-soma[(soma$adult_ratio<0.2),]
# Finally delete two variables we've just made -  adult_ratio, tweet_sum
soma <- soma[,-c(1,2)]
# Center and scale the data with 33 variables
soma_sc <- scale(soma, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data
mu= attr(soma_sc, "scaled:center")
sigma = attr(soma_sc, "scaled:scale")
# What is the optimal K?
# Elbow plot
k_grid = seq(2,20, by=1)
# CH index
N=nrow(soma)
# Method 1: Only K-means++
# Now do K-means++ with K=3
clust1 = kmeanspp(soma_sc, k=3, nstart=25)
clust1$center
length(which(clust1$cluster == 1))
length(which(clust1$cluster == 2))
length(which(clust1$cluster == 3))
sort(clust1$centers[1,], decreasing=TRUE)
sort(clust1$centers[2,], decreasing=TRUE)
sort(clust1$centers[3,], decreasing=TRUE)
set.seed(12)#set seed in order to get same clusters whenever we try again
clust2 = kmeanspp(soma_sc, k=4, nstart=25)
clust2$center
length(which(clust2$cluster == 1))
length(which(clust2$cluster == 2))
length(which(clust2$cluster == 3))
length(which(clust2$cluster == 4))
sort(clust2$centers[1,], decreasing=TRUE)
sort(clust2$centers[2,], decreasing=TRUE)
sort(clust2$centers[3,], decreasing=TRUE)
sort(clust2$centers[4,], decreasing=TRUE)
Cluster = factor(clust2$cluster)
subset(soma, select = c("family","school","food","sports_fandom","religion","parenting")) %>%
ggpairs(legend = 1, aes(color=Cluster, alpha = 0.6),
upper = list(integer = wrap("cor", size=2, alignPercent=0.8))) +
theme_bw() + theme(legend.position = "bottom", panel.grid = element_blank())
subset(soma, select = c("computers","travel","politics","news","automotive")) %>%
ggpairs(legend = 1, aes(color=Cluster, alpha = 0.6),
upper = list(integer = wrap("cor", size=2, alignPercent=0.8))) +
theme_bw() + theme(legend.position = "bottom", panel.grid = element_blank())
social_marketing <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv", row.names = 1)
# Get rid of spam users - Nobody tweets spam messages unless (s)he's a bot
soma<-soma[(soma$spam==0),] # delete observations related to spam
soma <- social_marketing
# Get rid of spam users - Nobody tweets spam messages unless (s)he's a bot
soma<-soma[(soma$spam==0),] # delete observations related to spam
soma <- soma[,-35] # get rid of spam variable
# we also delete variables "chatter" and "uncategorized"
# because the company cannot get any information from these categories
soma<- soma[,-c(1,5)]
# But we cannot delete all the users in adult category because it can be their hobby
# Then how can we distinguish bots from users?
# We meausure adult ratio
soma <- cbind(tweet_sum = rowSums(soma), soma)
soma <- cbind(adult_ratio = 1, soma)
soma$adult_ratio <- soma$adult/soma$tweet_sum
summary(soma$adult_ratio)
hist(soma$adult_ratio)
# Though there's no absolute criterion, We'll set 20% as our criterion
# Normal people don't tweet porn in public
# And their other non-adult tweets might lead other users into porn
length(which(soma$adult_ratio>0.2))
length(which(soma$adult_ratio>0.2))/(length(which(soma$adult_ratio>0)))
soma<-soma[(soma$adult_ratio<0.2),]
# Finally delete two variables we've just made -  adult_ratio, tweet_sum
soma <- soma[,-c(1,2)]
# Center and scale the data with 33 variables
soma_sc <- scale(soma, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data
mu= attr(soma_sc, "scaled:center")
sigma = attr(soma_sc, "scaled:scale")
set.seed(12)#set seed in order to get same clusters whenever we try again
clust2 = kmeanspp(soma_sc, k=4, nstart=25)
clust2$center
length(which(clust2$cluster == 1))
length(which(clust2$cluster == 2))
length(which(clust2$cluster == 3))
length(which(clust2$cluster == 4))
sort(clust2$centers[1,], decreasing=TRUE)
sort(clust2$centers[2,], decreasing=TRUE)
sort(clust2$centers[3,], decreasing=TRUE)
sort(clust2$centers[4,], decreasing=TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(foreach)
library(tidyverse)
library(LICORS)
library(cluster)
library(corrplot)
library(GGally)
social_marketing <- read.csv("https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW3/master/social_marketing.csv", row.names = 1)
soma<- social_marketing
#Cleaning the data
soma<-soma[(soma$spam==0),]
soma <- soma[,-35]
soma<- soma[,-c(1,5)]
soma <- cbind(tweet_sum = rowSums(soma), soma)
soma <- cbind(adult_ratio = 1, soma)
soma$adult_ratio <- soma$adult/soma$tweet_sum
soma<-soma[(soma$adult_ratio<0.2),]
soma <- soma[,-c(1,2)]
set.seed(12)#set seed in order to get same clusters whenever we try again
clust2 = kmeanspp(soma_sc, k=4, nstart=25)
Cluster = factor(clust2$cluster)
clust2$centers
length(which(clust2$cluster == 1))
length(which(clust2$cluster == 2))
length(which(clust2$cluster == 3))
length(which(clust2$cluster == 4))
subset(soma, select = c("family","school","food","sports_fandom","religion","parenting")) %>%
ggpairs(legend = 1, aes(color=Cluster, alpha = 0.6),
upper = list(integer = wrap("cor", size=2, alignPercent=0.8))) +
theme_bw() + theme(legend.position = "bottom", panel.grid = element_blank())
set.seed(12)#set seed in order to get same clusters whenever we try again
clust2 = kmeanspp(soma_sc, k=4, nstart=25)
Cluster = factor(clust2$cluster)
set.seed(12)#set seed in order to get same clusters whenever we try again
clust2 = kmeanspp(soma_sc, k=4, nstart=25)
Cluster = factor(clust2$cluster)
length(which(clust2$cluster == 1))
length(which(clust2$cluster == 2))
length(which(clust2$cluster == 3))
length(which(clust2$cluster == 4))
sort(clust2$centers[2,], decreasing=TRUE)
sort(clust2$centers[3,], decreasing=TRUE)
sort(clust2$centers[4,], decreasing=TRUE)
Cluster = factor(clust2$cluster)
subset(soma, select = c("family","school","food","sports_fandom","religion","parenting")) %>%
ggpairs(legend = 1, aes(color=Cluster, alpha = 0.6),
upper = list(integer = wrap("cor", size=2, alignPercent=0.8))) +
theme_bw() + theme(legend.position = "bottom", panel.grid = element_blank())
qplot(health_nutrition, religion, data=soma, color=factor(clust2$cluster))
qplot(news, parenting, data=soma, color=factor(clust2$cluster))
summary(pca)
pca= prcomp(soma_sc)#don't need to scale it because we already did
summary(pca)
### Can we use PCA in this example?
Now, we try to perform a principal component analysis to find low-dimensional summaries of our data set. Then we'll divide our samples into several clusters hoping that we can make our model simpler and more interpretable.
However, the table below shows that it's hard to narrow down the number of variables by using PCA. We need at least 7 principal components to explain more than 50% of data, 13 components for 70%, and 23 components for 90%.   So performing clustering analysis after PCA doesn't seem to be a good idea.
loadings[,c(1,4)]
loadings[,c(1:4)]
loadings= pca$rotation
scores = pca$x
loadings[,c(1:4)]
loadings[,c(1:3)]
?prcomp
o1 = order(loadings[,1], decreasing=TRUE)
colnames(soma_sc)[head(o1,3)]
head(scores[,1])
# pc1: the more interest you have in religion, food, parenting, the more scores you get
#      the less interest you have in college and university, online gaming, porn, the more scores you get
o2 = order(loadings[,2], decreasing=TRUE)
colnames(soma_sc)[head(o2,3)]
colnames(soma_sc)[tail(o2,3)]
# pc2: the more interest you have in sports fandom, religion, parenting, the more scores you get
#      the less interest you have in photo sharing, fashion, cooking, the more scores you get
o3 = order(loadings[,3], decreasing=TRUE)
colnames(soma_sc)[head(o3,3)]
colnames(soma_sc)[tail(o3,3)]
qplot(scores[,1], scores[,2], color=factor(clust2$cluster), xlab='Component1', ylab='Component2')
# cluster1: normal
# cluster2: low pc2 score -> likes photo sharing, fashion, cooking
#           We already saw that this type likes cooking, which corresponds with our clustering result.
# cluster3: high pc2 score -> likes sports fandom, religion, parenting
#           We already saw that this type likes religion, parenting and sports fandom, which perfectly corresponds with our clustering result.
# cluster4: low pc1 score -> likes college and university, online gaming and porn
#           We don't see these characteristics in our previous clustering analysis.
qplot(scores[,1], scores[,3], color=factor(clust2$cluster), xlab='Component1', ylab='Component3')
install.packages("tidytable")
library(tidytable)
# Method 1: Only K-means++
# Now do K-means++ with K=3
set.seed(1)
clust1 = kmeanspp(soma_sc, k=3, nstart=25)
clust1$center
length(which(clust1$cluster == 1))
length(which(clust1$cluster == 2))
length(which(clust1$cluster == 3))
# Method 1: Only K-means++
# Now do K-means++ with K=3
set.seed(2)
clust1 = kmeanspp(soma_sc, k=3, nstart=25)
clust1$center
length(which(clust1$cluster == 1))
# Method 1: Only K-means++
# Now do K-means++ with K=3
set.seed(3)
clust1 = kmeanspp(soma_sc, k=3, nstart=25)
clust1$center
length(which(clust1$cluster == 1))
# Method 1: Only K-means++
# Now do K-means++ with K=3
set.seed(4)
clust1 = kmeanspp(soma_sc, k=3, nstart=25)
clust1$center
length(which(clust1$cluster == 1))
length(which(clust1$cluster == 2))
View(soma)
blablalblab
length(which(clust1$cluster == 1))
length(which(clust1$cluster == 2))
length(which(clust1$cluster == 3))
sort(clust1$centers[1,], decreasing=TRUE)
?sort
sort(clust1$centers[2,], decreasing=TRUE)
sort(clust1$centers[3,], decreasing=TRUE)
kk = sort(clust1$centers[3,], decreasing=TRUE)
kk[,c(1:5)]
kk[1:5]
clust1_cent = sort(clust1$centers[1,], decreasing=TRUE)
clust1_cent[1:6]
sort(clust2$centers[2,], decreasing=TRUE)
clust1_cent = sort(clust1$centers[1,], decreasing=TRUE)
clust1_cent[1:6]
clust2_cent = sort(clust1$centers[2,], decreasing=TRUE)
clust2_cent[1:5]
clust3_cent = sort(clust1$centers[3,], decreasing=TRUE)
clust3_cent[1:5]
set.seed(12)#set seed in order to get same clusters whenever we try again
clust2 = kmeanspp(soma_sc, k=4, nstart=25)
clust2$center
length(which(clust2$cluster == 1))
length(which(clust2$cluster == 2))
length(which(clust2$cluster == 3))
length(which(clust2$cluster == 4))
clust1_cen = sort(clust2$centers[1,], decreasing=TRUE)
clust1_cen[1:5]
clust2_cen = sort(clust2$centers[2,], decreasing=TRUE)
clust2_cen[1:5]
clust3_cen = sort(clust2$centers[3,], decreasing=TRUE)
clust3_cen[1:5]
clust4_cen = sort(clust2$centers[4,], decreasing=TRUE)
clust4_cen[1:5]
stargazer(model1, single.row = TRUE, type='text', omit.stat = c("ser", "rsq","f"),
digits.extra = 1)
