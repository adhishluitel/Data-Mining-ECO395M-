---
title: "Homework3"
author: "Bernardo Magalhaes, Adhish Luitel, Ji Heon Shim"
date: "`r format(Sys.Date())`" 
always_allow_html: true
output:
    md_document:
    variant: markdown_github
---
#ECO 395M: Exercise 3

Bernardo Arreal Magalhaes - UTEID ba25727

Adhish Luitel - UTEID al49674

Ji Heon Shim - UTEID js93996

## Exercise 3.1
```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gamlr)
library(tidyverse)
library(kableExtra)
library(randomForest)
library(gbm)
library(pdp)
library(stargazer)
library(xtable)
library(tidytable)
greenbuildings = read.csv(url("https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW3/master/greenbuildings.csv"))

# data cleaning
colSums(is.na(greenbuildings))
grb= na.omit(greenbuildings)# find missing values and get rid of them
grb$size = grb$size/1000 # lower the scale of size in order to get around limits of computation
# We see an error occurs when doing a lasso regression if we don't lower the scale
```

In this exercise, we analyzed a dataset on green buildings to build the best predictive pricing model. We started with cleaning the data. First we detected all the null values that were missing and deleted them. As we are running a lasso regression, in order to comply with  the limits of computation, we scaled down 'size' variable from 'square footage' to '(square footage)/1000'.

Next, we built a base model and used step-wise selection. From the insights we gathered while cleaning up the data, we decided to delete the variable 'CS_PropertyID' as it was just a unique identity number and contributed nothing to our model. We also deleted another variable 'total_dd_07' due to the nature of its collinearity with the variables 'cd_total_07' and 'hd_total07'(total_dd_07 = cd_total_07 + hd_total07). Lastly, we also deleted the variable 'cluster' from our model because it was recognized as a numerical variable though it was a categorical one. And in order to reflect the effect of cluster on rent, we already have cluster.rent variable which shows the average rent by clusters.

Finally, in order to check if a building is a green building, we used only 'green_rating' as our dummy variable and didn't  consider 'LEED' and 'EnergyStar' separately.

```{r 3.1.1, echo= FALSE, warning = FALSE, include = FALSE, cache = TRUE}
base_model = lm(Rent~(.-CS_PropertyID-LEED-Energystar-total_dd_07-cluster), data=grb)
full = lm(Rent~(.-CS_PropertyID-LEED-Energystar-total_dd_07-cluster)^2, data=grb)
null = lm(Rent~1, data=grb)
# Forward Selection
system.time(fwd <- step(null, scope=formula(full), dir= "forward"))
length(coef(fwd))
# AIC 34512.51, 50 variables, elapsed time 36 sec

# Backward Selection
system.time(back <- step(full, dir="backward"))
length(coef(back))
# AIC 34372.28(the lowest!), 84 variables, elapsed time 13 min 58 sec
system.time(stepwise <- step(base_model, scope= list(lower=null, upper=full), dir='both'))
length(coef(stepwise))
# AIC 34407.55, 68 variables, elapsed time 3 min 5 sec
```

To find the best predictive model possible for price, we built 5 different models and compared their performances. At the same time, we measured elapsed time while we were running each model to see its computational efficiency. 

### Stepwise Selection Model
First, we used stepwise regression method to find the model with the best performance. 
We built forward selection model, backward selection model and stepwise selection model. i) Forward selection model starts with a model having no variables, and add all possible one-variable additions to it including every interaction. ii) Backward selection model starts with the full model that has all the variables including all of interactions, then improves its performance by deleting each variable. iii) Stepwise selection model starts with our base model 'lm(Rent~(.-CS_PropertyID-LEED-Energystar-total_dd_07-cluster)' and we considered all possible one-variable addtions or deletions including interactions. 

```{r 3.1.2, echo=FALSE, include = FALSE}
# Pick the backward selection model as our best model
model1 = lm(Rent ~ size + empl_gr + leasing_rate + stories + age + renovated + 
               class_a + class_b + green_rating + net + amenities + cd_total_07 + 
               hd_total07 + Precipitation + Gas_Costs + Electricity_Costs + 
               cluster_rent + size:leasing_rate + size:stories + size:age + 
               size:renovated + size:class_a + size:class_b + size:cd_total_07 + 
               size:hd_total07 + size:Electricity_Costs + size:cluster_rent + 
               empl_gr:age + empl_gr:class_b + empl_gr:Gas_Costs + leasing_rate:cd_total_07 + 
               leasing_rate:hd_total07 + leasing_rate:Precipitation + leasing_rate:Gas_Costs + 
               leasing_rate:Electricity_Costs + leasing_rate:cluster_rent + 
               stories:age + stories:renovated + stories:class_a + stories:class_b + 
               stories:amenities + stories:cd_total_07 + stories:Precipitation + 
               stories:Electricity_Costs + stories:cluster_rent + age:class_a + 
               age:class_b + age:green_rating + age:cd_total_07 + age:hd_total07 + 
               age:cluster_rent + renovated:cd_total_07 + renovated:hd_total07 + 
               renovated:Precipitation + renovated:Gas_Costs + renovated:Electricity_Costs + 
               renovated:cluster_rent + class_a:amenities + class_a:cd_total_07 + 
               class_a:hd_total07 + class_a:Precipitation + class_a:Gas_Costs + 
               class_a:Electricity_Costs + class_b:cd_total_07 + class_b:hd_total07 + 
               class_b:Precipitation + class_b:Gas_Costs + class_b:Electricity_Costs + 
               green_rating:amenities + net:cd_total_07 + net:cluster_rent + 
               amenities:Precipitation + amenities:Gas_Costs + amenities:Electricity_Costs + 
               amenities:cluster_rent + cd_total_07:Gas_Costs + cd_total_07:Electricity_Costs + 
               hd_total07:Precipitation + hd_total07:Gas_Costs + hd_total07:Electricity_Costs + 
               Precipitation:Gas_Costs + Precipitation:Electricity_Costs + 
               Electricity_Costs:cluster_rent, data=grb)

# Do additional stepwise selection based on model1
step(model1, scope = list(lower= null, upper=full), dir="both")
# No more progress! so we can say model1 is the best model when we use stepwise selection
```

The table below shows the performance measured by AIC, elapsed time and the number of variables of each model. As we can see, the backward selection model gives us the minimum AIC of 34372.28 with 84 variables, but it took very long time to compute all these procedures. 
In terms of AIC, we concluded that the backward selection model showed the best performance among three and ran an additional stepwise selection based on it to check if we could get any improvements. Since we didn't witness a further minimized AIC, we concluded that the backward selection model is out best model when we used stepwise selection. 

```{r 3.1.3, echo=FALSE, warning=FALSE, include=FALSE}
v <- c('AIC', 'Variables', 'Elapsed Time')
Forward_selection <- c(34512.51, 50, '36sec')
Backward_selection <- c(34372.28, 84, '13min 58sec')
Stepwise_selection <- c(34407.55, 68, '3min 5sec')
stepwise_result = data.frame(v, Forward_selection, Backward_selection, Stepwise_selection)

kable(stepwise_result) %>% kable_styling("striped")

# Do additional stepwise selection based on model1
step(model1, scope = list(lower= null, upper=full), dir="both")
# No more progress! so we can say model1 is the best model when we use a stepwise selection
```

Here's our best predictive stepwise selection model with 84 variables obtained by backward selection.

```{r 3.1.4, echo=FALSE}
stargazer(model1, single.row = TRUE, type='text', omit.stat = c("ser", "rsq","f"),
          digits.extra = 1)
```

```{r 3.1.5, echo=FALSE}
# Now do K-fold cross validation to check RMSE when K=10

# Create a vector of fold indicators
N=nrow(grb)
K=10
fold_id = rep_len(1:K, N) # repeats 1:K over and over again
fold_id = sample(fold_id, replace = FALSE) # permute the order randomly

# split train and test set and calculate RMSE for the stepwise selection model
err_save = rep(0, K)
for(i in 1:K){
  train_set = which(fold_id != i)
  y_test = grb$Rent[-train_set]
  model1_train = lm(model1, data=grb[train_set,])
  yhat_test = predict(model1_train, newdata = grb[-train_set,])
  err_save[i] = mean((y_test-yhat_test)^2)
}

```
Finally, we did K-fold cross validation to check RMSE when K is 10. We built a train-test split and repeated the step from 1 to K repetitions by running a loop. When we calculate RMSE for the backward selection model, it turned out to be `r round(sqrt(mean(err_save)),2)`.

```{r 3.1.5.1, echo=TRUE}
#RMSE
sqrt(mean(err_save))

```

### Lasso Regression Model
After this, we fit a lasso regression model to attempt to assemble the best predictive model. We used our full model including all the variables and interactions except some variable that we mentioned above - CS_PropertyID, LEED, Energystar, total_dd_07, cluster.
Running the lasso regression model, the path plot is shown on the diagram below.

```{r 3.1.6, echo=FALSE}
grb_x = sparse.model.matrix(Rent~(.-CS_PropertyID-LEED-Energystar-total_dd_07-cluster)^2, data=grb)[, -1]
grb_y = grb$Rent
grblasso = gamlr(grb_x, grb_y)
plot(grblasso) 
```

When we measure AICcs of all the segments, the 100th segment has the lowest AIC value of 34644.64. 

```{r 3.1.7, echo=TRUE, warning=FALSE}

min(AICc(grblasso))
which.min(AICc(grblasso))
```

Our optimal value of lambda turns out to be -2.17 in log scale, and at the optimal lambda, our lasso regression model has 25 variables with an intercept.The result below shows the coefficients of grb_beta, the minimum lambda in log scale, and the total number of variables including an intercept.

```{r 3.1.8, echo=FALSE}
grb_beta = coef(grblasso) 

grb_variables = as.data.frame(grb_beta@Dimnames[[1]])
var_position = c(grb_beta@i + 1)
grb_result = grb_variables[var_position,1]
grb_result = grb_result[, drop=TRUE]
var_coef = as.data.frame(grb_result)
var_coef[] <- lapply(var_coef, as.character)
var_coef = cbind(var_coef, c(grb_beta@x))
colnames(var_coef) = c("Variable","Coefficient")

kable(var_coef) %>% kable_styling("striped")

log(grblasso$lambda[which.min(AICc(grblasso))])
sum(grb_beta!=0)

```
 
```{r 3.1.9, echo=FALSE}
p1 <- dimnames(grb_beta)[[1]]
p2 <- c()
for (i in c(1:length(grb_beta))){
  p2 <- c(p2, as.list(grb_beta)[[i]])
}
model2 = c("Rent ~ ")
for (i in c(2:length(grb_beta))){
  if (p2[i] != 0){
    if (model2 == "Rent ~ "){
      model2 = paste(model2, p1[i])
    }
    else{
      model2 = paste(model2,"+", p1[i])
    }
  }
}
model2 <- as.formula(model2)
model2 = lm(model2, data=grb)

err_save2 = rep(0, K)
for(i in 1:K){
  train_set2 = which(fold_id != i)
  y_test2 = grb$Rent[-train_set2]
  model2_train = lm(model2, data=grb[train_set2,])
  yhat_test2 = predict(model2_train, newdata = grb[-train_set2,])
  err_save2[i] = mean((y_test2-yhat_test2)^2)
}
``` 

Then we did K-fold cross validation on our lasso regression model as well. We found that the root mean squared error for our lasso model is `r round(sqrt(mean(err_save2)),2)`, which was a lot higher than for our stepwise selection model, so we can say that the stepwise selection model shows better performance than the lasso regression model. 
However, the lasso model takes almost 0 seconds to compute all the procedures to derive its best model whereas the stepwise method takes more than 10 minutes to do the same thing. Therefore, we can say the lasso model is computationally more efficient than the stepwise selection model. 



```{r 3.1.9.1, echo=TRUE}
#RMSE
sqrt(mean(err_save2))
```

### Tree - Bagging Model

```{r 3.1.10, echo=FALSE, warning = FALSE, include=FALSE, cache=TRUE}
err_save3 = rep(0, K)
system.time(for(i in 1:K){
  train_set3 = which(fold_id != i)
  y_test3 = grb$Rent[-train_set3]
  model3_train = randomForest(Rent~(.-CS_PropertyID-LEED-Energystar-total_dd_07-cluster), data=grb[train_set3,], mtry = 17)
  yhat_test3 = predict(model3_train, newdata = grb[-train_set3,])
  err_save3[i] = mean((y_test3-yhat_test3)^2)
})
```

To further polish our best prediction model, we tried treebagging our best model with K-fold validation to assess its performance. We used our base model('Rent~(.-CS_PropertyID-LEED-Energystar-total_dd_07-cluster') for our tree bagging, and excluded all the interaction variables because we didn't need them explicity in tree regressions. The minimum size of terminal nodes is 5 which is default setting.
We set the number of trees to be 300 times and run the regression. It took more than 7 minutes to compute the whole procedure, however, the RMSE for our bagging model was `r round(sqrt(mean(err_save3)), 2)` which was much lower than those of our previous models.

 
```{r 3.1.10.1, echo=TRUE}
#RMSE
sqrt(mean(err_save3))
```
 
Below, we constructed a plot that shows the error in the variables. Judging by the relation between the error and the number of trees, 300 trees which we used in our model is large enough to reduce our errors.


```{r 3.1.11, echo=FALSE, warning = FALSE}
plot(model3_train, main = 'Bagging Model') 
```

In addition, the graph below shows a variable importance plot. The bigger the number is ,the greater the reduction on RMSE we get. We can see that "cluster_rent" variable has the most important impact on decideing the rent price in our tree bagging model.

```{r 3.1.12, echo=FALSE}
varImpPlot(model3_train, main='Bagging Model')
```

```{r 3.1.13, echo=FALSE, include=FALSE, cache=TRUE}
err_save4 = rep(0, K)
system.time(for(i in 1:K){
  train_set4 = which(fold_id != i)
  y_test4 = grb$Rent[-train_set4]
  model4_train = randomForest(Rent~(.-CS_PropertyID-LEED-Energystar-total_dd_07-cluster), data=grb[train_set4,], ntree=300)
  yhat_test4 = predict(model4_train, newdata = grb[-train_set4,])
  err_save4[i] = mean((y_test4-yhat_test4)^2)
})
```

### Tree - Random Forest Model
Now, we fit a random forest model to predict rent price using our base model and also did K-fold cross validation. First, we start with 300 trees and it took almost 3 minutes to compute all the procedures, however, the plot below shows that the error curve stops decreasing much after 50 trees. So we can reduce our number of trees to 100 and save our computational time.

```{r 3.1.13.1, echo=FALSE }
plot(model4_train, main = 'Random Forest Model')
```

```{r 3.1.13.2, include=FALSE, echo=FALSE, cache=TRUE}
err_save4 = rep(0, K)
system.time(for(i in 1:K){
  train_set4 = which(fold_id != i)
  y_test4 = grb$Rent[-train_set4]
  model4_train = randomForest(Rent~(.-CS_PropertyID-LEED-Energystar-total_dd_07-cluster), data=grb[train_set4,], ntree=100)
  yhat_test4 = predict(model4_train, newdata = grb[-train_set4,])
  err_save4[i] = mean((y_test4-yhat_test4)^2)
})
```

Now we fit a random forest model with 100 trees and do K-fold cross validation again. The result indicates that the value of RMSE is `r round(sqrt(mean(err_save4)), 2)` which is the smallest of all the RMSEs of the models we've seen above. And the computational efficiency has been improved by taking less than 1 minute to compute all these procedures.

```{r 3.1.13.3, echo=FALSE}
model4_train
```

```{r 3.1.13.4, echo=TRUE}
#RMSE
sqrt(mean(err_save4))
```

The plot below is the variable importance plot. We can see that 'cluster_rent' is the most important variable influencing rent price, which is the same result as we saw in the bagging model. But it reveals a slight difference in that the third influential variable is 'Electricity costs', which is 'age' in bagging.

```{r 3.1.14, echo=FALSE}
varImpPlot(model4_train, main = 'Random Forest Model')
```

### Tree - Boosting
```{r 3.1.15, echo=FALSE, warning=FALSE, include=FALSE, cache=TRUE}
err_save5 = rep(0, K)

system.time(for(i in 1:K){
  train_set5 = which(fold_id != i)
  y_test5 = grb$Rent[-train_set5]
  model5_train <- gbm(Rent~(.-CS_PropertyID-LEED-Energystar-total_dd_07-cluster), data=grb[train_set5,], interaction.depth=4, n.trees = 300, shrinkage =.05)
  yhat_test5 = predict(model5_train, newdata = grb[-train_set5,], n.trees =300)
  err_save5[i] = mean((y_test5-yhat_test5)^2)
})

```

Finally, we fit a boosting model to derive the best predictive model. As we've done before, we use our base model to begin with. The result of our K-fold cross validation shows that the RMSE is `r round(sqrt(mean(err_save5)),2)` which is slightly higher than that of our random forest model. However, it took only 15 seconds to compute all these procedures.


```{r 3.1.16, echo=TRUE}
#RMSE
sqrt(mean(err_save5))
```

Here is the summary of our boosting model which shows the relative influences of all variables. It appears to be 'cluster_rent', 'size', 'leasing_rate' are three most influential variables on our dependent variable. This result is similar with previous results that we've seen above, but the third influential variable is slightly different, too.

```{r 3.1.17, echo=TRUE}
summary(model5_train)
```

### Which model shows the best performance?

By using K-fold cross validation, we derived 5 RMSE out of 5 models as below.
We can see that the randomforest model shows the lowest RMSE which means the best performance.  

The randomforest model is superior in terms of computational speed. It took less than 1 minute to compute all the procedures, which is very efficient compared to the stepwise selection and the bagging. The lasso regression and the boosting method didn't take much computational time, but their performances are worse than the random forest model.Therefore, we can conclude that the random forest model shows the best performance.

```{r 3.1.18, echo=FALSE, warning=FALSE}
Model <- c('Stepwise', 'Lasso','Bagging','Randomforest','Boosting')
RMSE <- c(round(sqrt(mean(err_save)),2), round(sqrt(mean(err_save2)),2),round(sqrt(mean(err_save3)),2),round(sqrt(mean(err_save4)),2),round(sqrt(mean(err_save5)),2))
RMSE_result = data.frame(Model,RMSE)
RMSE_result = t(RMSE_result)
kable(RMSE_result) %>% kable_styling("striped")
```

### The Partial Effect of Green Certification on Rent
In order to derive the average change in rental income per square foot associated with green certification, holding other features of the building constant, we used 'partial' function in 'pdp package'.  

The average rent value without green certification holding other features constant is 28.50924, and the average rent value with green certification holding other features is 29.07784. Therefore, the difference 0.5686 is the average change in rental income per square foot associated with green certification, holding other feature of the building fixed.

```{r 3.1.19, echo=TRUE}
partial(model4_train, pred.var = 'green_rating', n.trees=100)
29.07784-28.50924
```
## Exercise 3.2

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)

table1_url = 'https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW3/master/HW3_1_files/3.2-table1.png'
table2_url = 'https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW3/master/HW3_1_files/3.2-table2.png'

```

### Why can’t I just get data from a few different cities  and  run  the  regression  of  "Crime" on "Police" to understand how more cops in the streets affect crime?

A researcher can easily think about a model that treats "Police" variables as regressors when explaining changes in crime, and another model, with a different research agenda, that includes "Crime" variables as regressors when explaining changes in police force. This fact suggests that crime and police might be jointly determined, what creates an endogeneity problem and brings difficulties when trying to disentangle the causal mechanism from the correlation between police force and crime, possibly misleading to wrong conclusions.

Therefore, if we collect data from different cities and run a regression of "Crime" on "Police", the coefficient associated with the police force might be biased. If more police officers are hired in response to an increase in crime, a positive correlation can emerge even if this increase in police force end up reducing crime after all. Also, cities that have a high level of criminality are likely to have large police forces, while cities that have a low level of criminality are likely to have a small police force. Detroit, for instance, has twice as many police officers per capita as Omaha, and a violent crime rate over four times as high, but it would be a mistake to attribute the differences in crime rates to the presence of the police. (Levitt, 1995)

### How were the researchers from UPenn able to isolate this effect?

In order to isolate the causal effect, the researchers explored how an exogenous change in police force affected crimes in the District of Columbia. They argue that, in the presence of a high terrorism alert, there is an increase in police force that is not caused by local crimes. So, given the fact that (i) terrorism risk is correlated with police force (instrument relevance: $Cov(z,x) \neq 0$) and (ii) terrorism risk only affects local crime indirectly thru the police force (instrument exogeneity: $Cov(z,u) = 0$), the researchers could exploit the variation in terrorism alert as an exogenous factor that affects crime.

```{r 3.2.1, echo= FALSE, warning = FALSE}
include_graphics(table1_url)
```

Column 1 shows that the presence of a high alert is associated with a decrease of 7.3 crimes per day, holding all else fixed. Column 2 shows that, when controlling by the presence of tourists (using log of Metro ridership at midday), the presence of a high alert is associated with a decrease of 6.2 crimes per day, holding all else fixed. Both coefficients are statistically significant at the 5% level.

### Why did they have to control for Metro ridership? What was that trying to capture?

The researchers controlled for Metro ridership because there is a reasonable possibility that a decrease in crime as a response to the presence of a high terrorism alert might be happening indirectly thru alternative factors, such as a reduction in tourism. In that sense, midday Metro ridership is a good proxy for tourism, and this hypothesis implies that having fewer potential victims on the streets would lead to fewer committed crimes.

When controlling for midday Metro ridership, the magnitude of the coefficient associated with a high alert slightly decrease, but remained in the confidence interval ($-7.316 \pm 2.877 \times 1.96$). Column 2 shows that a 10% increase in Metro ridership is associated with an increase of 1.7 crimes per day, holding all else fixed.

This procedure is important because, when trying to measure the causal effect of an intervention, we are implicitly assuming that all the other variables that affect the outcome, observed and unobserved, are not changing. Comparative statics methodological concept describes the effect of an explanatory variable on the response variable when holding all else fixed. (Cunningham, 2020) So, if something else is changing, we need to control for that change. 

### Below I am showing you "Table 4" from the researchers' paper. Just focus on the first column of the table. Can you describe the model being estimated here? What is the conclusion?

```{r 3.2.2, echo= FALSE, warning = FALSE}
include_graphics(table2_url)
```

The researchers also analyzed the geographical patterns of crime in Washington, D.C. Given the fact that most of the potential targets of terrorist attacks, such as the White House and the Congress, are located in the National Mall area, one could argue that, in the presence of a terror alert, the police department would divert resources from other districts and concentrate them in District 1. If this hypothesis was right, we would expect a decrease in crime in District 1, but an increase in crime in the other districts. So, the researchers included an interaction $High Alert \times District1$ to allow for district fixed effects.

Column 1 shows that, holding all else fixed, crime in District 1 decreases by 2.62 crimes per day, while in the other districts the effect is not statistically significant (but also not positive). This result provide evidence that an important part of the decrease in crime associated with the presence of a high terror alert is driven by the decrease in crime in the National Mall area, where the increment of police force tends to be placed.
```{r setup3, include=FALSE}
library(tidyverse)
library(LICORS)
library(ISLR)
library(foreach)
library(mosaic)
library(GGally)
library(corrplot)
library(ggplot2)
library(DescTools)
wine_data <- "https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv"
wine <- read.csv(url(wine_data))
```
### Exercise 3.3

In this question we analyzed a data set on 6,500 different bottles of vinho verde wine from northern Portugal. We had information on 11 chemical properties of these wines, whether they were red or white and the quality of the wine, as judged on a 1-10 scale. Our goal was to assess what dimension reduction clustering technique (k-means or PCA) would be more accurate. We were also tasked to determine if chemical properties could be used as indicators to determine the quality of wines. 

### Data cleaning
We started out by cleaning the data first. We centered, scaled and normalized the data to mitigate the impacts of extreme data points. 
```{r 3.3.1, echo= FALSE, warning = FALSE, include = FALSE, cache = TRUE}
z = wine[,1:11]
z = scale(z, center=TRUE, scale=TRUE)
z_std = z
mu = attr(z_std,"scaled:center")
sigma = attr(z_std,"scaled:scale")
res <- cor(z_std)
```
Then we plotted the below shown correlation plot of all 11 chemical properties to determine how strongly they are correlated with one another. Just by eyeballing the below plot, we could tell that perhaps the chemical properties “total sulfur dioxide”, “free sulfur dioxide” and “residual sugar” would be the most relevant chemical properties for us. 
```{r 3.3.2, echo= FALSE, warning = FALSE, include = TRUE, cache = TRUE}
corrplot(res, type = 'lower', method = "color", order = "hclust", hclust.method = "ward.D", tl.cex = 0.5, tl.col="black")
```
Then we started clustering our normalized dataset with k-means 2 as we had 2 types of wines by color, red or white and with 25 starts. We then attempted to find the scaled center of one of the clusters in order to find the most prominent variables, judging by their coefficient. When we did this for cluster 1, we found out that the chemical properties “total sulfur dioxide”, “free sulfur dioxide” and “residual sugar” appeared to have the highest coefficients. This implied the other cluster (cluster 2) would have the lowest coefficient values for these variables, so these were the variables could explain the difference between the two different wine color clusters very clearly. This further cemented our reasoning for picking these variables. 
```{r 3.3.3, echo= FALSE, warning = FALSE, include = FALSE, cache = TRUE}
#Clustering
#Start with K means 2 as we have two basic categories (Red/White) and 25 starts
cluster1 = kmeans(z_std, 2, nstart=25) 

#Picking variables
sort(cluster1$centers[1,], decreasing=TRUE)
sort(cluster1$centers[2,], decreasing=TRUE)

#Comparing 
Cluster = factor(cluster1$cluster)
```
From the below scatter plots and bell curves of these three chemical properties we could see that they formed somewhat proper clusters, however there were plenty of overlaps. We also needed to determine that our two clusters had distinguished wine types, red and white.
```{r 3.3.4, echo= FALSE, warning = FALSE, include = TRUE, cache = TRUE}
subset(wine, select = c("total.sulfur.dioxide","free.sulfur.dioxide","residual.sugar")) %>%
  ggpairs(legend = 1, aes(color=Cluster, alpha = 0.6),
          upper = list(integer = wrap("cor", size=2, alignPercent=0.8))) +
  theme_bw() + theme(legend.position = "bottom", panel.grid = element_blank())
```
After this we attempted to form the clusters as per the colors of the wines. So the cluster that mostly had red wines was be the red wine cluster and similarly the cluster that mostly had white wines was be the white wine cluster. 
```{r 3.3.5, echo= FALSE, warning = FALSE, include = TRUE, cache = TRUE}
wine$cluster = cluster1$cluster
wine = wine %>%
  group_by(cluster) %>%
  mutate(color_hat = Mode(color))


table1 = xtabs(~wine$color_hat + wine$color) 
print(table1)
```
From the above table we could see that we had clustered our wines pretty accurately, with a majority of our red wines being under the ‘color_hat’ cluster red and white wines being under the ‘color_hat’ cluster white. With a 98.58% accuracy, we concluded that our k-means clustering had done a pretty good job in terms of dimension reduction. 
```{r 3.3.6, echo= FALSE, warning = FALSE, include = TRUE, cache = TRUE}
cluster_kpp = kmeanspp(z_std, k=2, nstart=25)

wine$cluster_kpp = cluster_kpp$cluster
wine = wine %>%
  group_by(cluster_kpp) %>%
  mutate(color_hat_kpp = Mode(color))

table2 = xtabs(~wine$color_hat_kpp + wine$color) ###
print(table2)
```
Next, we did a k -means++ clustering with k-means 2 and 25 starts, again. Just like before, we labelled our clusters as ‘color_hat’ cluster red and white wines being under the ‘color_hat’ cluster white. As shown above, with a 98.58% accuracy, we found out that our k-means++ clustering performed just as efficiently as k-means. 
```{r 3.3.7, echo= FALSE, warning = FALSE, include = TRUE, cache = TRUE}
pca = prcomp(z_std, scale=TRUE)
summary(pca)
```
Next, we performed a Principal Component Analysis(PCA). The above table showed that the first three principal components (PC1-PC3) combined form 64.3% of the total variance in the dataset, which is a highly significant proportion. Based on this, we used our first three principal components to perform our clustering.
```{r 3.3.8, echo= FALSE, warning = FALSE, include = TRUE, cache = TRUE}
scores = pca$x
loadings = pca$rotation


cluster_pca = kmeans(scores[,1:3], 2, nstart=25)
qplot(scores[,1], scores[,2], data=wine, color=factor(cluster_pca$cluster))

table3 = xtabs(~cluster_pca$cluster + wine$color) 
print(table3)
```
From the above scatter plot and confusion matrix we could see that our PCA based clustering also did a very good job in terms of dimension reduction with an accuracy rate of 98.3%. Hence, we concluded that in terms of this case, k-means, k-means++ and PCA all seem to be excellent dimension reduction techniques, however k-means and k-means++ outperformed PCA by a mere 0.2%p.

### Distinguishing quality
After this we were tasked with answering another key problem; whether this technique also seem capable of sorting the higher from the lower quality wines or not. Due to this we had to distinguish the quality of wine, since all the wines were judged from a scale of 1-10, we had 10 categories. However, inspecting the data set, we found out that a majority of wines were rated 5 or 6 and there were no wines that were scored 1, 2 or 10, that is extremely high or low. Thus, in reality, we only had 7 different qualities of wine. Bearing this in mind, we applied the same tools as we did initially.
```{r 3.3.9, echo= FALSE, warning = FALSE, include = TRUE, cache = TRUE}
cluster2 = kmeans(z_std, 7, nstart=25)
wine$cluster2 = cluster2$cluster
table_wine = wine %>%
  group_by(cluster2) %>%
  summarize(quality_3 = sum(quality == 3),
            quality_4 = sum(quality == 4),
            quality_5 = sum(quality == 5),
            quality_6 = sum(quality == 6),
            quality_7 = sum(quality == 7),
            quality_8 = sum(quality == 8),
            quality_9 = sum(quality == 9))
table4 = xtabs(~cluster2$cluster + wine$quality)
print(table4)
```
Above, we see the confusion matrix of our k-means clustering which barely provided any insight.  
```{r 3.3.10, echo= FALSE, warning = FALSE, include = TRUE, cache = TRUE}
cluster_kpp2 = kmeanspp(z_std, 7, nstart=25)

table5 = xtabs(~cluster_kpp2$cluster + wine$quality)
print(table5)
```
Similarly, we conducted k-means++ clustering. Just as the case with k-means clustering, the confusion matrix in this case didn't provide much insights either. 
```{r 3.3.11, echo= FALSE, warning = FALSE, include = TRUE, cache = TRUE}
pca2 = prcomp(z_std, scale=TRUE)
loadings = pca2$rotation
scores = pca2$x
cluster_pca1 = kmeans(scores[,1:3], 7, nstart=25)
qplot(scores[,1], scores[,2], data=wine, color=factor(cluster_pca1$cluster), xlab='Component 1', ylab='Component 2')
```
Similarly, we also did a PCA based clustering and noticed that bar few extremes, all the data points are clustered collectively around the center, as we could see in the scatterplot that illustrated all 7 clusters. Based on this we decided to see the absolute quantities of wines that were given their respective scores. 

As seen in the above table, we could see how the quality scores of the wines are distributed and where our problem lied. A majority of wines were rated between 5, 6 and 7. That meant, all 7 clusters would be dominated by wines being rated between 5-7. As a result, wines rated 2, 3 or 9 were heavily under-represented whereas wines rated between 5-7 were heavily over-represented among all 7 clusters. Due to this it was determined to distinguish what chemical property played a role in good or poor quality of wines. Earlier, when we were only distinguishing between red and white wines using k-means 2, we had some overlaps, but this wasn’t much of an issue and our results were still pretty accurate as we had only two categories and two clusters. But as the number of categories and clusters grew, the disparity between categories was more prevalent and we couldn’t succeed in sorting the higher from the lower quality wines.

## Exercise 3.4
```{r setup4, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(foreach)
library(tidyverse)
library(LICORS)
library(cluster)
library(corrplot)
library(GGally)

social_marketing <- read.csv("https://raw.githubusercontent.com/bmagalhaes/ECO395M-HW3/master/social_marketing.csv", row.names = 1)
soma<- social_marketing

#Cleaning the data
soma<-soma[(soma$spam==0),]
soma <- soma[,-35]
soma<- soma[,-c(1,5)]
soma <- cbind(tweet_sum = rowSums(soma), soma)
soma <- cbind(adult_ratio = 1, soma)
soma$adult_ratio <- soma$adult/soma$tweet_sum
soma<-soma[(soma$adult_ratio<0.2),]
soma <- soma[,-c(1,2)]
```

In this exercise, we explored the content of multiple tweets from a sample of followers of NutrientH20's Twitter profile, and come up with some interesting insights about the audience by identifying possible market segments.

### Data cleaning
We started by cleaning our data set, which is originally comprised of 7,882 observations and 36 variables.

First, we deleted all the users whose tweet fell into 'spam' category at least once, due to the high probability of this account being a bot. Naturally, we excluded the 'spam' variable from our data set.

Additionally, we also deleted the variables 'chatter' and 'uncategorized',  since these categories would hardly give some useful insights about the followers.

Finally, even though tweets classified in the 'adult' category can be interpreted as spam, sometimes it can also be a hobby for real followers too. In that sense, considering that normal people don't tweet much porn in public, we deleted users that have more than 20% of their total posts in pornography, regarding them as bots.  
So we end up with 7,676 observations with 33 variables in our data set.  
  
### K-means++ clustering
We'll start with performing clustering analysis. Since our data doesn't show any kinds of hierarchy, we'll focus on K-means analysis. We'll use K-means++ rather than K-means because it's already known that K-means++ shows a better performance.  
The first question we encounter when doing K-means++ is "How many clusters should we set?", i.e, "What is our optimal K?".  
In order to find its answer, we used three methods - Elbow plot, CH index, and Gap statistics.  


```{r 3.4.1, echo= FALSE, warning = FALSE, include = FALSE, cache = TRUE}
# Extract the centers and scales from the rescaled data
soma_sc <- scale(soma, center=TRUE, scale=TRUE)
mu= attr(soma_sc, "scaled:center")
sigma = attr(soma_sc, "scaled:scale")

# What is the optimal K? Elbow plot
k_grid = seq(2,20, by=1)
SSE_grid = foreach(k=k_grid, .combine='c') %do% {cluster_k= kmeanspp(soma_sc, k, nstart=25)
cluster_k$tot.withinss}

N=nrow(soma)
CH_grid= foreach(k=k_grid, .combine='c') %do% {
  cluster_k = kmeanspp(soma_sc, k, nstart=25)
  W= cluster_k$tot.withinss
  B= cluster_k$betweenss
  CH= (B/W)*((N-k)/(k-1))
  CH
}

soma_gap = clusGap(soma_sc, FUN= kmeanspp, nstart=25, K.max=10, B=100)
```

We scaled our data first, and performed three analyses by using scaled data.  
The first graph shows the elbow plot of our data set. As we can see, there's no distinguished elbow here, so it doesn't give us clear messages about the optimal K.  
The second one is the CH index. We can see it is the highest when K=1, and it decreases gradually. So we cannot get a satisfactory answer for our question, either.  


```{r 3.4.2, echo= FALSE, warning = FALSE}
plot(k_grid, SSE_grid) #cannot see any elbow here!
plot(k_grid, CH_grid)# K=2 or 3?
```

Finally, we can see our gap statistics diagram of our data below. However, We cannot find any dips here. Unfortunately, it seems that none of these means could lead us to a conclusive answer for the optimal K.

```{r 3.4.3, echo= FALSE, warning = FALSE}
plot(soma_gap)
```

Since it is difficult to find the optimal K only by eyeballing the graph, we decided to plot a correlogram, and try to identify possible singularities among the variables. 

```{r 3.4.4, echo= FALSE, warning = FALSE}
#correlation matrix
res <- cor(soma_sc)
corrplot(res, type = 'lower', method = "color", order = "hclust", hclust.method = "ward.D", tl.cex = 0.5, tl.col="black")
```

From the correlation plot, we can observe some subgroups of variables that are highly correlated with each other. The variables 'family', 'school', 'food', 'sports_fandom', 'religion' form the first group. 'Computers', 'travel', 'politics', 'news' and 'automotive' form the second. 'Outdoors', 'health_nutrition' and 'personal_fitness' the third. 'Sports_playing', 'online_gaming' and 'college_uni' form the fourth. And 'beauty', 'cooking' and 'fashion' form the fifth.

Therefore, we decided to use K=5.

```{r 3.4.5, echo= FALSE, warning = FALSE}
set.seed(12)#set seed in order to get same clusters whenever we try again
clust2 = kmeanspp(soma_sc, k=5, nstart=25)
Cluster = factor(clust2$cluster)
```

```{r 3.4.6, echo= FALSE, warning = FALSE, cache = TRUE}
subset(soma, select = c("family","school","food","sports_fandom","religion","parenting")) %>%
  ggpairs(legend = 1, aes(color=Cluster, alpha = 0.6),
        upper = list(integer = wrap("cor", size=0.5, alignPercent=0.8))) +
  theme_bw() + theme(legend.position = "bottom", panel.grid = element_blank())
```

For the first subgroup, we can see that followers that belong to "Cluster 4" are those that, in general, have a high interest in 'family', 'school', 'food', 'sports_fandom' and 'religion'.

```{r 3.4.7, echo= FALSE, warning = FALSE, cache = TRUE}
subset(soma, select = c("computers","travel","politics","news","automotive")) %>%
  ggpairs(legend = 1, aes(color=Cluster, alpha = 0.6),
          upper = list(integer = wrap("cor", size=0.5, alignPercent=0.8))) +
  theme_bw() + theme(legend.position = "bottom", panel.grid = element_blank())
```

For the second subgroup, we can see that followers that belong to "Cluster 1" are those that, in general, have a high interest in 'Computers', 'travel', 'politics', 'news' and 'automotive'.

```{r 3.4.8, echo= FALSE, warning = FALSE, cache = TRUE}
subset(soma, select = c("outdoors","health_nutrition","personal_fitness")) %>%
  ggpairs(legend = 1, aes(color=Cluster, alpha = 0.6),
          upper = list(integer = wrap("cor", size=0.5, alignPercent=0.8))) +
  theme_bw() + theme(legend.position = "bottom", panel.grid = element_blank())
```

For the third subgroup, we can see that followers that belong to "Cluster 3" are those that, in general, have a high interest in  ‘Outdoors’, ‘health_nutrition’ and ‘personal_fitness’.

```{r 3.4.9, echo= FALSE, warning = FALSE, cache = TRUE}
subset(soma, select = c("beauty","cooking","fashion")) %>%
  ggpairs(legend = 1, aes(color=Cluster, alpha = 0.6),
          upper = list(integer = wrap("cor", size=0.5, alignPercent=0.8))) +
  theme_bw() + theme(legend.position = "bottom", panel.grid = element_blank())
```

For the fourth subgroup, we can see that followers that belong to "Cluster 5" are those that, in general, have a high interest in  ‘Sports_playing’, ‘online_gaming’ and ‘college_uni’.

```{r 3.4.10, echo= FALSE, warning = FALSE, cache = TRUE}
subset(soma, select = c("sports_playing","online_gaming","college_uni")) %>%
  ggpairs(legend = 1, aes(color=Cluster, alpha = 0.6),
          upper = list(integer = wrap("cor", size=0.5, alignPercent=0.8))) +
  theme_bw() + theme(legend.position = "bottom", panel.grid = element_blank())
```

For the fifth subgroup, we can't find some well defined patterns when comparing within cluster interests in ‘beauty’, ‘cooking’ and ‘fashion’.

### Can we use PCA in this example?
Now, we try to perform a principal component analysis to find low-dimensional summaries of our data set. Then we'll divide our samples into several clusters hoping that we can make our model simpler and more interpretable.  
However, the table below shows that it's hard to narrow down the number of variables by using PCA. We need at least 7 principal components to explain more than 50% of the variance of the data, 13 components for 70%, and 23 components for 90%.  
So performing clustering analysis after PCA doesn't seem to be a good idea. However, PCA itself can give us some useful information about our samples in another sense.

```{r 3.4.11, echo=FALSE}

pca= prcomp(soma_sc)#don't need to scale it because we already did
summary(pca)

loadings= pca$rotation
scores = pca$x
```

The table below is the matrix of variable loadings from PC1 to PC4. We'll see if what we found by PCA can share something in common with our findings by our K-means++ clustering.
In PC1, we can see 'religion', 'food', and 'parenting' show the largest value, which means the more tweets a user has made in these categories, the more scores (s)he'll get in PC1. And there is no negative value here.  
In PC2, 'sports_fandom', 'religion', 'parenting' have the largest values, which means the more tweets one has made in these categories, the more scores one will get in PC2.
On the other hand, 'cooking','fashion', and 'photo_sharing' show the smallest values, and they are all negative numbers. It means that the more one tweets in these categories, the less scores one will get.
In PC3, 'politics','travel', and 'computers' have the largest values, which means the more tweets one has made in these categories, the more scores one will get in PC3. On the other hand, 'helath_nutrition', 'personal_fitness', and 'cooking' have the smallest and negative values, which means the more tweets one has made in these categories, the less scores one will get in PC3.  
Lastly, in PC4, 'health_nutrition','personal_fitness','outdoors' have the largest values, which means the more tweets one has made in these categories, the more scores one will get in PC4. On the other hand, 'college_uni', 'online_gaming', and 'sport_playing' have the smallest and negatieve values, which means the more tweets one has made in these categories, the less scores one will get in PC4.

```{r 3.4.12, echo=FALSE}
loadings[,c(1:4)]
```
```{r 3.4.13, echo=FALSE, include=FALSE}
o1 = order(loadings[,1], decreasing=TRUE)
colnames(soma_sc)[head(o1,3)]
colnames(soma_sc)[tail(o1,3)]
o2 = order(loadings[,2], decreasing=TRUE)
colnames(soma_sc)[head(o2,3)]
colnames(soma_sc)[tail(o2,3)]
o3 = order(loadings[,3], decreasing=TRUE)
colnames(soma_sc)[head(o3,3)]
colnames(soma_sc)[tail(o3,3)]
o4 = order(loadings[,4], decreasing=TRUE)
colnames(soma_sc)[head(o4,3)]
colnames(soma_sc)[tail(o4,3)]
```

Now, We plot our samples on two-dimensional space which consist of two axis - PC1 and PC2, and color them by clusters that we showed in Kmeans++. Here, we can find different characteristics depending on clusters.
For example, cluster4 clearly shows higher PC2 values than other clusters do on average. We've already seen that users in cluster4 have a high interest in 'sports_fandom' and 'religion.' And as we have shown above, the more tweets one has made in 'sports_fandom' and 'religion', the more scores one will get in PC2. So it is no wonder that cluster4 shows high PC2 values on the graph below.  

```{r 3.4.14, echo=FALSE}
qplot(scores[,1], scores[,2], color=factor(clust2$cluster), xlab='Component1', ylab='Component2')
```

We can find the commonality between clustering and PCA in another example. The plot below shows the relationship between PC3 and PC4. One thing we can clearly see here is that cluster1 shows higher PC3 value than other clusters on average.  
We already mentioned above that the more tweets one has made in 'politics','travel', and 'computers' categories, the more PC3 scores one will get. And we can recall that users in cluster1 have a high interest in 'computers', 'travel', and 'politics'. This correspondence fortifies our finding that clustering and PCA can share something in common.

```{r 3.4.15, echo=FALSE}
qplot(scores[,3], scores[,4], color=factor(clust2$cluster), xlab='Component3', ylab='Component4')
```
